#!/usr/bin/env python3
"""
HolmesGPT Service
è´Ÿè´£ HolmesGPT çš„åˆå§‹åŒ–ã€é…ç½®å’ŒæŸ¥è¯¢æ‰§è¡Œ
"""
import os
import json
import logging
import time
from pathlib import Path
from typing import Optional, Tuple, Any, Generator, Dict
from datetime import datetime

from rich.console import Console

from holmes.config import Config
from holmes.core.prompt import build_initial_ask_messages
from holmes.plugins.runbooks import RunbookCatalog
from holmes.utils.stream import StreamEvents, StreamMessage

from app.core.runbook import RunbookManager, get_project_root
from app.core.prompts import SYSTEM_PROMPT
from app.core.environment import get_config_file_path, log_environment_info, get_environment

logger = logging.getLogger(__name__)


def create_sse_message_cn(event_type: str, data: Optional[Dict] = None) -> str:
    """
    åˆ›å»º SSE æ¶ˆæ¯ï¼Œæ”¯æŒä¸­æ–‡è¾“å‡ºï¼ˆä¸è½¬ä¹‰ä¸º Unicodeï¼‰
    
    Args:
        event_type: äº‹ä»¶ç±»å‹
        data: äº‹ä»¶æ•°æ®
    
    Returns:
        SSE æ ¼å¼çš„æ¶ˆæ¯å­—ç¬¦ä¸²
    """
    if data is None:
        data = {}
    # ensure_ascii=False ç¡®ä¿ä¸­æ–‡ä¸è¢«è½¬ä¹‰
    return f"event: {event_type}\ndata: {json.dumps(data, ensure_ascii=False)}\n\n"


def format_duration(seconds: float) -> str:
    """æ ¼å¼åŒ–æŒç»­æ—¶é—´ä¸ºäººç±»å¯è¯»æ ¼å¼"""
    if seconds < 1:
        return f"{seconds*1000:.0f}ms"
    elif seconds < 60:
        return f"{seconds:.1f}s"
    else:
        minutes = int(seconds // 60)
        secs = seconds % 60
        return f"{minutes}m {secs:.1f}s"


class HolmesService:
    """HolmesGPT æœåŠ¡ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        self.config: Optional[Config] = None
        self.ai: Any = None
        self.console = Console()
        self.runbook_manager = RunbookManager()
        self.merged_catalog: Optional[RunbookCatalog] = None
        self.stream_output: bool = False  # æµå¼è¾“å‡ºé…ç½®
    
    def initialize(
        self,
        api_key: Optional[str] = None,
        model: Optional[str] = None,
        max_steps: int = 50,
        config_file: Optional[Path] = None
    ) -> Tuple[Config, Any]:
        """
        åˆå§‹åŒ– HolmesGPT é…ç½®å’Œ AI å®ä¾‹
        
        Args:
            api_key: LLM API Key
            model: ä½¿ç”¨çš„æ¨¡å‹
            max_steps: æœ€å¤§æ‰§è¡Œæ­¥æ•°
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        
        Returns:
            (config, ai_instance) å…ƒç»„
        """
        # å¦‚æœå·²ç»åˆå§‹åŒ–ï¼Œç›´æ¥è¿”å›
        if self.config is not None and self.ai is not None:
            return self.config, self.ai
        
        logger.info("åˆå§‹åŒ– HolmesGPT é…ç½®...")
        
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        project_root = get_project_root()
        
        # è·å–é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆè‡ªåŠ¨æ£€æµ‹ç¯å¢ƒï¼‰
        if config_file is None:
            config_file, env_name = get_config_file_path(project_root)
            logger.info(f"ğŸ”§ è¿è¡Œç¯å¢ƒ: {env_name}")
        
        # å…ˆè¯»å–æµå¼è¾“å‡ºé…ç½®ï¼ˆåœ¨ Config.load_from_file ä¹‹å‰ï¼Œé¿å…éªŒè¯é”™è¯¯ï¼‰
        if config_file.exists():
            self._load_stream_config(config_file)
        else:
            self.stream_output = False
        
        # ç¡®å®šä½¿ç”¨çš„ API Key
        final_api_key = api_key or os.getenv("DEEPSEEK_API_KEY") or os.getenv("OPENAI_API_KEY")
        if not final_api_key:
            raise ValueError(
                "æœªæä¾› API Keyï¼Œè¯·é€šè¿‡å‚æ•°æˆ–ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY/OPENAI_API_KEY è®¾ç½®"
            )
        
        # ç¡®å®šä½¿ç”¨çš„æ¨¡å‹
        final_model = model or "deepseek/deepseek-chat"
        
        # åŠ è½½é…ç½®ï¼ˆéœ€è¦å…ˆåˆ›å»ºä¸€ä¸ªä¸´æ—¶é…ç½®æ–‡ä»¶ï¼Œç§»é™¤ stream_output å­—æ®µï¼‰
        if config_file.exists():
            logger.info(f"ä»é…ç½®æ–‡ä»¶åŠ è½½: {config_file}")
            # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶ï¼ˆç§»é™¤ stream_output å­—æ®µï¼‰
            import yaml
            import tempfile
            with open(config_file, 'r', encoding='utf-8') as f:
                config_dict = yaml.safe_load(f)
            
            # ç¯å¢ƒå˜é‡æ›¿æ¢ï¼šæ”¯æŒ ${VAR} å’Œ ${VAR:-default} è¯­æ³•
            config_dict = self._substitute_env_vars(config_dict)
            logger.info("âœ… ç¯å¢ƒå˜é‡æ›¿æ¢å®Œæˆ")
            
            # ç§»é™¤ stream_output å­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            temp_config_dict = {k: v for k, v in config_dict.items() if k != "stream_output"}
            
            # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶ï¼ˆç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„ YAML æ ¼å¼ï¼‰
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False, encoding='utf-8') as temp_file:
                yaml.dump(temp_config_dict, temp_file, allow_unicode=True, default_flow_style=False, sort_keys=False)
                temp_config_path = Path(temp_file.name)
            
            try:
                self.config = Config.load_from_file(
                    config_file=temp_config_path,
                    api_key=final_api_key,
                    model=final_model,
                    max_steps=max_steps
                )
            finally:
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                temp_config_path.unlink()
        else:
            logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            self.config = Config(
                api_key=final_api_key,
                model=final_model,
                max_steps=max_steps
            )
        
        # åŠ è½½å’Œåˆå¹¶ runbook catalogs
        self._load_runbooks()
        
        # åˆ›å»º AI å®ä¾‹
        logger.info("åˆ›å»º AI å®ä¾‹...")
        self.ai = self.config.create_console_toolcalling_llm()
        
        # é…ç½®è‡ªå®šä¹‰ runbook æœç´¢è·¯å¾„
        if self.runbook_manager.runbook_dir.exists():
            self.runbook_manager.configure_search_path(self.ai)
        
        logger.info(f"âœ… HolmesGPT åˆå§‹åŒ–å®Œæˆï¼Œæ¨¡å‹: {self.config.model}")
        logger.info(f"ğŸ“¡ è¾“å‡ºæ¨¡å¼: {'æµå¼è¾“å‡º (stream)' if self.stream_output else 'éæµå¼è¾“å‡º (invoke)'}")
        
        # è¾“å‡ºåŠ è½½çš„èµ„æºä¿¡æ¯
        self._log_loaded_resources()
        
        return self.config, self.ai
    
    def _load_stream_config(self, config_file: Path):
        """
        ä»é…ç½®æ–‡ä»¶åŠ è½½æµå¼è¾“å‡ºé…ç½®
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        try:
            import yaml
            with open(config_file, 'r', encoding='utf-8') as f:
                config_dict = yaml.safe_load(f)
            
            # è¯»å– stream_output é…ç½®ï¼ˆé¡¶çº§å­—æ®µï¼‰
            self.stream_output = config_dict.get("stream_output", False)
            
            output_mode = "æµå¼è¾“å‡º (stream)" if self.stream_output else "éæµå¼è¾“å‡º (invoke)"
            logger.info(f"ğŸ“¡ è¾“å‡ºæ¨¡å¼: {output_mode}")
        except Exception as e:
            logger.warning(f"è¯»å–æµå¼è¾“å‡ºé…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆéæµå¼ï¼‰: {e}")
            self.stream_output = False
    
    def _substitute_env_vars(self, obj: Any, depth: int = 0) -> Any:
        """
        é€’å½’æ›¿æ¢é…ç½®ä¸­çš„ç¯å¢ƒå˜é‡å ä½ç¬¦
        
        æ”¯æŒè¯­æ³•:
            - ${VAR}         - ä½¿ç”¨ç¯å¢ƒå˜é‡ VAR çš„å€¼ï¼Œä¸å­˜åœ¨åˆ™ä¿ç•™åŸå­—ç¬¦ä¸²
            - ${VAR:-default} - ä½¿ç”¨ç¯å¢ƒå˜é‡ VAR çš„å€¼ï¼Œä¸å­˜åœ¨åˆ™ä½¿ç”¨ default
        
        Args:
            obj: è¦å¤„ç†çš„å¯¹è±¡ï¼ˆdictã€list æˆ– strï¼‰
            depth: é€’å½’æ·±åº¦ï¼ˆé˜²æ­¢æ— é™é€’å½’ï¼‰
        
        Returns:
            æ›¿æ¢åçš„å¯¹è±¡
        """
        import re
        
        if depth > 50:  # é˜²æ­¢æ— é™é€’å½’
            return obj
        
        if isinstance(obj, dict):
            return {k: self._substitute_env_vars(v, depth + 1) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._substitute_env_vars(item, depth + 1) for item in obj]
        elif isinstance(obj, str):
            # åŒ¹é… ${VAR} æˆ– ${VAR:-default}
            pattern = r'\$\{([A-Za-z_][A-Za-z0-9_]*)(?::-([^}]*))?\}'
            
            def replace_match(match):
                var_name = match.group(1)
                default_value = match.group(2)  # å¯èƒ½æ˜¯ None
                
                env_value = os.environ.get(var_name)
                
                if env_value is not None:
                    logger.debug(f"ğŸ”„ ç¯å¢ƒå˜é‡æ›¿æ¢: ${{{var_name}}} -> ***")
                    return env_value
                elif default_value is not None:
                    logger.debug(f"ğŸ”„ ä½¿ç”¨é»˜è®¤å€¼: ${{{var_name}}} -> {default_value}")
                    return default_value
                else:
                    # ç¯å¢ƒå˜é‡ä¸å­˜åœ¨ä¸”æ²¡æœ‰é»˜è®¤å€¼ï¼Œä¿ç•™åŸå­—ç¬¦ä¸²ï¼ˆä½†è®°å½•è­¦å‘Šï¼‰
                    logger.warning(f"âš ï¸ ç¯å¢ƒå˜é‡æœªè®¾ç½®: {var_name}")
                    return match.group(0)  # ä¿ç•™åŸå­—ç¬¦ä¸²
            
            return re.sub(pattern, replace_match, obj)
        else:
            return obj
    
    def _call_with_stream(self, messages: list) -> Any:
        """
        ä½¿ç”¨æµå¼è¾“å‡ºè°ƒç”¨ AIï¼Œæ”¶é›†æ‰€æœ‰äº‹ä»¶åè¿”å›æœ€ç»ˆå“åº”
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
        
        Returns:
            ä¸ ai.call() ç›¸åŒæ ¼å¼çš„å“åº”å¯¹è±¡
        """
        from holmes.utils.stream import StreamEvents
        
        # ä» messages ä¸­æå– system_prompt å’Œ user_prompt
        system_prompt = ""
        user_prompt = None
        msgs = []
        
        for msg in messages:
            if msg.get("role") == "system":
                system_prompt = msg.get("content", "")
            elif msg.get("role") == "user":
                if user_prompt is None:
                    user_prompt = msg.get("content", "")
                else:
                    msgs.append(msg)
            else:
                msgs.append(msg)
        
        # è°ƒç”¨æµå¼è¾“å‡º
        final_result = None
        final_tool_calls = []
        all_content = []
        
        try:
            for stream_event in self.ai.call_stream(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                msgs=msgs if msgs else None
            ):
                # æ”¶é›† AI æ¶ˆæ¯å†…å®¹
                if stream_event.event == StreamEvents.AI_MESSAGE:
                    content = stream_event.data.get("content", "")
                    if content:
                        all_content.append(content)
                
                # æ”¶é›†å·¥å…·è°ƒç”¨ç»“æœ
                elif stream_event.event == StreamEvents.TOOL_RESULT:
                    tool_data = stream_event.data
                    if tool_data:
                        tool_name = tool_data.get("name") or tool_data.get("tool_name") or "unknown"
                        result_dict = tool_data.get("result", {})
                        
                        if isinstance(result_dict, dict):
                            result_str = result_dict.get("data") or str(result_dict)
                            error_str = result_dict.get("error")
                        else:
                            result_str = str(result_dict)
                            error_str = None
                        
                        tool_info = {
                            "tool_name": tool_name,
                            "result": str(result_str) if result_str else None,
                            "error": str(error_str) if error_str else None
                        }
                        final_tool_calls.append(tool_info)
                
                # æ£€æŸ¥æ˜¯å¦ç»“æŸ
                elif stream_event.event == StreamEvents.ANSWER_END:
                    final_result = "".join(all_content)
                    break
            
            # å¦‚æœæ²¡æœ‰æ”¶åˆ° ANSWER_ENDï¼Œä½¿ç”¨æ”¶é›†åˆ°çš„å†…å®¹
            if final_result is None:
                final_result = "".join(all_content)
            
            # åˆ›å»ºä¸€ä¸ªç±»ä¼¼ response çš„å¯¹è±¡
            class StreamResponse:
                def __init__(self, result, tool_calls):
                    self.result = result
                    self.tool_calls = tool_calls
            
            return StreamResponse(final_result, final_tool_calls)
            
        except Exception as e:
            logger.error(f"æµå¼è¾“å‡ºå¤„ç†å¤±è´¥: {e}", exc_info=True)
            logger.warning("å›é€€åˆ°éæµå¼è¾“å‡º")
            return self.ai.call(messages)
    
    def _load_runbooks(self):
        """åŠ è½½å’Œåˆå¹¶ runbook catalogs"""
        # åŠ è½½è‡ªå®šä¹‰ runbook catalog
        custom_catalog = self.runbook_manager.load_custom_catalog()
        
        # è·å–å†…ç½® runbook catalog
        base_catalog = self.config.get_runbook_catalog()
        
        # åˆå¹¶ catalogs
        self.merged_catalog = self.runbook_manager.merge_catalogs(
            base_catalog, custom_catalog
        )
    
    def _log_loaded_resources(self):
        """è¾“å‡ºåŠ è½½çš„èµ„æºä¿¡æ¯ï¼ˆå·¥å…·é›†ã€MCP æœåŠ¡å™¨ã€å·¥å…·ã€Runbookï¼‰"""
        if not self.ai or not self.ai.tool_executor:
            return
        
        # 1. è¾“å‡ºå·¥å…·é›†ä¿¡æ¯ï¼ˆæŒ‰ç±»å‹åˆ†ç±»ï¼‰
        toolsets = self.ai.tool_executor.toolsets
        builtin_toolsets = []
        mcp_servers = []
        
        for toolset in toolsets:
            toolset_class_name = toolset.__class__.__name__.lower()
            is_mcp = (
                'mcp' in toolset_class_name or
                (hasattr(toolset, 'type') and str(toolset.type).lower() == 'mcp') or
                (hasattr(toolset, '__module__') and 'mcp' in toolset.__module__.lower())
            )
            
            if is_mcp:
                mcp_servers.append(toolset)
            else:
                builtin_toolsets.append(toolset)
        
        # è¾“å‡ºå†…ç½®å·¥å…·é›†
        if builtin_toolsets:
            enabled_builtin = [ts for ts in builtin_toolsets if ts.enabled]
            if enabled_builtin:
                successful_toolsets = []
                failed_toolsets = []
                for toolset in enabled_builtin:
                    toolset_tools = []
                    if hasattr(toolset, 'tools') and toolset.tools:
                        toolset_tools = [t.name for t in toolset.tools if hasattr(t, 'name')]
                    registered_tools = [t for t in toolset_tools if t in self.ai.tool_executor.tools_by_name]
                    
                    status_str = str(toolset.status.value) if hasattr(toolset.status, 'value') else str(toolset.status)
                    if status_str == "enabled" and registered_tools:
                        successful_toolsets.append((toolset, len(registered_tools)))
                    else:
                        failed_toolsets.append((toolset, status_str, len(toolset_tools), len(registered_tools)))
                
                if successful_toolsets:
                    logger.info(f"ğŸ“¦ å†…ç½®å·¥å…·é›† ({len(successful_toolsets)} ä¸ªå·²å¯ç”¨å¹¶å¯ç”¨):")
                    for toolset, tool_count in successful_toolsets:
                        logger.info(f"   âœ… {toolset.name} ({tool_count} ä¸ªå·¥å…·)")
                
                if failed_toolsets:
                    logger.warning(f"âš ï¸  å†…ç½®å·¥å…·é›† ({len(failed_toolsets)} ä¸ªé…ç½®ä½†æœªæˆåŠŸåŠ è½½):")
                    for toolset, status, total_tools, registered_tools in failed_toolsets:
                        error_msg = getattr(toolset, 'error', 'æœªçŸ¥é”™è¯¯')
                        logger.warning(f"   âŒ {toolset.name} (çŠ¶æ€: {status}, å·¥å…·: {registered_tools}/{total_tools}, é”™è¯¯: {error_msg[:100]})")
        
        # è¾“å‡º MCP æœåŠ¡å™¨
        if mcp_servers:
            enabled_mcp = [ts for ts in mcp_servers if ts.enabled]
            if enabled_mcp:
                logger.info(f"ğŸŒ MCP æœåŠ¡å™¨ ({len(enabled_mcp)} ä¸ªå·²è¿æ¥):")
                for toolset in enabled_mcp:
                    tool_count = len(toolset.tools) if hasattr(toolset, 'tools') else 0
                    status_icon = "âœ…" if toolset.status.value == "enabled" else "âŒ"
                    logger.info(f"   {status_icon} {toolset.name} ({tool_count} ä¸ªå·¥å…·)")
            else:
                logger.info("ğŸŒ MCP æœåŠ¡å™¨: æ— å·²å¯ç”¨çš„æœåŠ¡å™¨")
        else:
            logger.info("ğŸŒ MCP æœåŠ¡å™¨: æœªé…ç½®")
        
        # 2. è¾“å‡ºå·¥å…·ç»Ÿè®¡
        all_tools = list(self.ai.tool_executor.tools_by_name.keys())
        if all_tools:
            tool_counts = {}
            for toolset in toolsets:
                if toolset.enabled and hasattr(toolset, 'tools'):
                    toolset_tools = [t.name for t in toolset.tools if hasattr(t, 'name')]
                    registered_tools = [t for t in toolset_tools if t in self.ai.tool_executor.tools_by_name]
                    if registered_tools:
                        tool_counts[toolset.name] = len(registered_tools)
            
            logger.info(f"ğŸ”§ å¯ç”¨å·¥å…·: æ€»è®¡ {len(all_tools)} ä¸ªï¼ˆå·²æ³¨å†Œï¼‰")
            if tool_counts:
                sorted_counts = sorted(tool_counts.items(), key=lambda x: x[1], reverse=True)[:10]
                for toolset_name, count in sorted_counts:
                    logger.info(f"   â€¢ {toolset_name}: {count} ä¸ªå·¥å…·")
                if len(tool_counts) > 10:
                    logger.info(f"   ... è¿˜æœ‰ {len(tool_counts) - 10} ä¸ªå·¥å…·é›†")
        
        # 3. è¾“å‡º Runbook ä¿¡æ¯
        if self.merged_catalog and self.merged_catalog.catalog:
            logger.info(f"ğŸ“š Runbook çŸ¥è¯†åº“: {len(self.merged_catalog.catalog)} ä¸ª")
            for entry in self.merged_catalog.catalog[:5]:
                if hasattr(entry, 'title'):
                    title = entry.title
                elif isinstance(entry, dict):
                    title = entry.get('title', 'Unknown')
                else:
                    title = str(entry)
                logger.info(f"   â€¢ {title}")
            if len(self.merged_catalog.catalog) > 5:
                logger.info(f"   ... è¿˜æœ‰ {len(self.merged_catalog.catalog) - 5} ä¸ª runbook")
        else:
            logger.info("ğŸ“š Runbook çŸ¥è¯†åº“: æœªé…ç½®")
    
    def execute_query(
        self,
        question: str,
        system_prompt: Optional[str] = None,
        api_key: Optional[str] = None,
        model: Optional[str] = None,
        max_steps: int = 50
    ) -> dict:
        """
        æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿”å›ç»“æœ
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            system_prompt: è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
            api_key: LLM API Key
            model: ä½¿ç”¨çš„æ¨¡å‹
            max_steps: æœ€å¤§æ‰§è¡Œæ­¥æ•°
        
        Returns:
            åŒ…å«æŸ¥è¯¢ç»“æœçš„å­—å…¸
        """
        start_time = datetime.now()
        
        try:
            # å¦‚æœå‚æ•°å˜åŒ–ï¼Œé‡æ–°åˆå§‹åŒ–
            if api_key or model or max_steps != 50:
                self.config = None
                self.ai = None
            
            # åˆå§‹åŒ–ï¼ˆå¦‚æœè¿˜æœªåˆå§‹åŒ–ï¼‰
            self.initialize(api_key=api_key, model=model, max_steps=max_steps)
            
            # ç¡®å®šä½¿ç”¨çš„ç³»ç»Ÿæç¤ºè¯
            final_system_prompt = system_prompt or SYSTEM_PROMPT
            logger.info(f"æ‰§è¡ŒæŸ¥è¯¢: {question[:100]}...")
            
            # ä½¿ç”¨åˆå¹¶åçš„ runbook catalog
            runbook_catalog = (
                self.merged_catalog if self.merged_catalog 
                else self.config.get_runbook_catalog()
            )
            
            # æ„å»ºæ¶ˆæ¯
            messages = build_initial_ask_messages(
                console=self.console,
                initial_user_prompt=question,
                file_paths=None,
                tool_executor=self.ai.tool_executor,
                runbooks=runbook_catalog,
                system_prompt_additions=final_system_prompt if final_system_prompt else None
            )
            
            # æ ¹æ®é…ç½®é€‰æ‹©è°ƒç”¨æ–¹å¼
            if self.stream_output:
                response = self._call_with_stream(messages)
            else:
                response = self.ai.call(messages)
            
            # æå–å·¥å…·è°ƒç”¨ä¿¡æ¯
            tool_calls = []
            if response and hasattr(response, 'tool_calls') and response.tool_calls:
                if isinstance(response.tool_calls, list) and len(response.tool_calls) > 0:
                    if isinstance(response.tool_calls[0], dict):
                        tool_calls = response.tool_calls
                    else:
                        for tool in response.tool_calls:
                            tool_calls.append({
                                "tool_name": tool.tool_name,
                                "result": str(tool.result) if hasattr(tool, 'result') else None,
                                "error": str(tool.error) if hasattr(tool, 'error') and tool.error else None
                            })
            
            execution_time = (datetime.now() - start_time).total_seconds()
            
            return {
                "success": True,
                "result": response.result if response else None,
            }
            
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"æ‰§è¡ŒæŸ¥è¯¢æ—¶å‡ºé”™: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "execution_time": execution_time,
                "timestamp": datetime.now().isoformat()
            }
    
    def execute_query_stream(
        self,
        question: str,
        system_prompt: Optional[str] = None,
        api_key: Optional[str] = None,
        model: Optional[str] = None,
        max_steps: int = 50,
        output_format: str = "text"
    ) -> Generator[str, None, None]:
        """
        æ‰§è¡ŒæŸ¥è¯¢å¹¶ä»¥æµå¼æ–¹å¼è¿”å›ç»“æœï¼ˆå¸¦è€—æ—¶ç»Ÿè®¡ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            system_prompt: è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
            api_key: LLM API Key
            model: ä½¿ç”¨çš„æ¨¡å‹
            max_steps: æœ€å¤§æ‰§è¡Œæ­¥æ•°
            output_format: è¾“å‡ºæ ¼å¼ - "text"=æ˜“è¯»çº¯æ–‡æœ¬, "sse"=JSONæ ¼å¼SSEäº‹ä»¶
        
        Yields:
            æ ¹æ® output_format è¿”å›çº¯æ–‡æœ¬æˆ– SSE æ ¼å¼çš„äº‹ä»¶å­—ç¬¦ä¸²
        """
        # æ ¹æ®è¾“å‡ºæ ¼å¼é€‰æ‹©è¾“å‡ºå‡½æ•°
        if output_format == "text":
            yield from self._execute_query_stream_text(
                question, system_prompt, api_key, model, max_steps
            )
            return
        
        # ==================== SSE æ ¼å¼è¾“å‡º ====================
        total_start_time = time.time()
        tool_calls_collected = []
        timing_stats = {
            "initialization": 0,
            "message_building": 0,
            "llm_iterations": [],
            "tool_calls": [],
            "total": 0
        }
        iteration_count = 0
        current_tool_start_time = None
        current_tool_name = None
        llm_iteration_start_time = None
        
        try:
            # åˆå§‹åŒ–é˜¶æ®µ
            init_start = time.time()
            
            if api_key or model or max_steps != 50:
                self.config = None
                self.ai = None
            
            self.initialize(api_key=api_key, model=model, max_steps=max_steps)
            timing_stats["initialization"] = time.time() - init_start
            
            final_system_prompt = system_prompt or SYSTEM_PROMPT
            
            logger.info("=" * 60)
            logger.info(f"ğŸ“ [æµå¼æŸ¥è¯¢] é—®é¢˜: {question[:100]}...")
            logger.info(f"â±ï¸  åˆå§‹åŒ–è€—æ—¶: {format_duration(timing_stats['initialization'])}")
            
            yield create_sse_message_cn("stream_start", {
                "message": "ğŸš€ å¼€å§‹å¤„ç†æŸ¥è¯¢...",
                "question": question[:100],
                "phase": "initialization",
                "init_time": format_duration(timing_stats["initialization"]),
                "timestamp": datetime.now().isoformat()
            })
            
            # æ¶ˆæ¯æ„å»ºé˜¶æ®µ
            msg_build_start = time.time()
            
            runbook_catalog = (
                self.merged_catalog if self.merged_catalog 
                else self.config.get_runbook_catalog()
            )
            
            messages = build_initial_ask_messages(
                console=self.console,
                initial_user_prompt=question,
                file_paths=None,
                tool_executor=self.ai.tool_executor,
                runbooks=runbook_catalog,
                system_prompt_additions=final_system_prompt if final_system_prompt else None
            )
            
            timing_stats["message_building"] = time.time() - msg_build_start
            logger.info(f"â±ï¸  æ¶ˆæ¯æ„å»ºè€—æ—¶: {format_duration(timing_stats['message_building'])}")
            
            # æå– prompts
            sys_prompt = ""
            user_prompt = None
            msgs = []
            
            for msg in messages:
                if msg.get("role") == "system":
                    sys_prompt = msg.get("content", "")
                elif msg.get("role") == "user":
                    if user_prompt is None:
                        user_prompt = msg.get("content", "")
                    else:
                        msgs.append(msg)
                else:
                    msgs.append(msg)
            
            # LLM è°ƒç”¨é˜¶æ®µ
            final_content = None
            llm_iteration_start_time = time.time()
            
            logger.info("-" * 60)
            logger.info("ğŸ¤– å¼€å§‹ LLM è¿­ä»£...")
            
            for stream_event in self.ai.call_stream(
                system_prompt=sys_prompt,
                user_prompt=user_prompt,
                msgs=msgs if msgs else None
            ):
                event_type = stream_event.event
                event_data = stream_event.data
                
                if event_type == StreamEvents.START_TOOL:
                    tool_name = event_data.get("tool_name", "unknown")
                    tool_id = event_data.get("id", "")
                    current_tool_start_time = time.time()
                    current_tool_name = tool_name
                    
                    logger.info(f"  ğŸ”§ [{iteration_count+1}] å¼€å§‹è°ƒç”¨å·¥å…·: {tool_name}")
                    
                    yield create_sse_message_cn("tool_start", {
                        "tool_name": tool_name,
                        "tool_id": tool_id,
                        "iteration": iteration_count + 1,
                        "message": f"ğŸ”§ æ­£åœ¨è°ƒç”¨å·¥å…·: {tool_name}",
                        "timestamp": datetime.now().isoformat()
                    })
                
                elif event_type == StreamEvents.TOOL_RESULT:
                    tool_name = event_data.get("name") or event_data.get("tool_name") or current_tool_name or "unknown"
                    result_dict = event_data.get("result", {})
                    description = event_data.get("description", "")
                    
                    tool_duration = 0
                    if current_tool_start_time:
                        tool_duration = time.time() - current_tool_start_time
                        timing_stats["tool_calls"].append({
                            "name": tool_name,
                            "duration": tool_duration,
                            "iteration": iteration_count + 1
                        })
                    
                    if isinstance(result_dict, dict):
                        result_str = result_dict.get("data") or str(result_dict)
                        error_str = result_dict.get("error")
                        status = result_dict.get("status", "unknown")
                    else:
                        result_str = str(result_dict)
                        error_str = None
                        status = "success"
                    
                    tool_info = {
                        "tool_name": tool_name,
                        "result": str(result_str)[:500] if result_str else None,
                        "error": str(error_str) if error_str else None,
                        "status": status,
                        "duration": tool_duration
                    }
                    tool_calls_collected.append(tool_info)
                    
                    status_icon = "âœ…" if status == "success" else "âŒ"
                    logger.info(f"  {status_icon} [{iteration_count+1}] å·¥å…·å®Œæˆ: {tool_name} (è€—æ—¶: {format_duration(tool_duration)})")
                    
                    yield create_sse_message_cn("tool_result", {
                        "tool_name": tool_name,
                        "description": description,
                        "status": status,
                        "result_preview": str(result_str)[:300] if result_str else None,
                        "error": str(error_str) if error_str else None,
                        "duration": format_duration(tool_duration),
                        "duration_seconds": round(tool_duration, 2),
                        "iteration": iteration_count + 1,
                        "timestamp": datetime.now().isoformat()
                    })
                    
                    current_tool_start_time = None
                    current_tool_name = None
                
                elif event_type == StreamEvents.AI_MESSAGE:
                    content = event_data.get("content", "")
                    reasoning = event_data.get("reasoning", "")
                    
                    if reasoning:
                        # æ—¥å¿—æ˜¾ç¤ºå®Œæ•´å†…å®¹ï¼ˆæ¢è¡Œç¬¦æ›¿æ¢ä¸ºç©ºæ ¼ä¾¿äºé˜…è¯»ï¼‰
                        log_reasoning = reasoning.replace('\n', ' ')
                        logger.info(f"  ğŸ’­ [{iteration_count+1}] AI æ¨ç†: {log_reasoning}")
                        yield create_sse_message_cn("ai_reasoning", {
                            "reasoning": reasoning,
                            "iteration": iteration_count + 1,
                            "timestamp": datetime.now().isoformat()
                        })
                    
                    if content:
                        # æ—¥å¿—æ˜¾ç¤ºå®Œæ•´å†…å®¹ï¼ˆæ¢è¡Œç¬¦æ›¿æ¢ä¸ºç©ºæ ¼ä¾¿äºé˜…è¯»ï¼‰
                        log_content = content.replace('\n', ' ')
                        logger.info(f"  ğŸ’¬ [{iteration_count+1}] AI æ¶ˆæ¯: {log_content}")
                        yield create_sse_message_cn("ai_message", {
                            "content": content,
                            "iteration": iteration_count + 1,
                            "timestamp": datetime.now().isoformat()
                        })
                
                elif event_type == StreamEvents.TOKEN_COUNT:
                    if llm_iteration_start_time:
                        iteration_duration = time.time() - llm_iteration_start_time
                        timing_stats["llm_iterations"].append({
                            "iteration": iteration_count + 1,
                            "duration": iteration_duration
                        })
                        logger.info(f"  â±ï¸  [{iteration_count+1}] è¿­ä»£å®Œæˆï¼Œè€—æ—¶: {format_duration(iteration_duration)}")
                    
                    iteration_count += 1
                    llm_iteration_start_time = time.time()
                    
                    metadata = event_data.get("metadata", {})
                    usage = metadata.get("usage", {})
                    if usage:
                        yield create_sse_message_cn("token_count", {
                            "usage": usage,
                            "iteration": iteration_count,
                            "elapsed_time": format_duration(time.time() - total_start_time),
                            "timestamp": datetime.now().isoformat()
                        })
                
                elif event_type == StreamEvents.CONVERSATION_HISTORY_COMPACTED:
                    logger.info(f"  ğŸ“¦ [{iteration_count+1}] å¯¹è¯å†å²å·²å‹ç¼©")
                    yield create_sse_message_cn("history_compacted", {
                        "message": "ğŸ“¦ å¯¹è¯å†å²å·²å‹ç¼©ä»¥é€‚åº”ä¸Šä¸‹æ–‡çª—å£",
                        "iteration": iteration_count + 1,
                        "timestamp": datetime.now().isoformat()
                    })
                
                elif event_type == StreamEvents.ANSWER_END:
                    final_content = event_data.get("content", "")
                    
                    if llm_iteration_start_time:
                        iteration_duration = time.time() - llm_iteration_start_time
                        timing_stats["llm_iterations"].append({
                            "iteration": iteration_count + 1,
                            "duration": iteration_duration
                        })
                    
                    logger.info(f"  ğŸ¯ [{iteration_count+1}] æ”¶åˆ°æœ€ç»ˆç­”æ¡ˆ")
                    break
                
                elif event_type == StreamEvents.ERROR:
                    error_msg = event_data.get("msg", "æœªçŸ¥é”™è¯¯")
                    logger.error(f"  âŒ [{iteration_count+1}] é”™è¯¯: {error_msg}")
                    yield create_sse_message_cn("error", {
                        "error": error_msg,
                        "iteration": iteration_count + 1,
                        "timestamp": datetime.now().isoformat()
                    })
                
                elif event_type == StreamEvents.APPROVAL_REQUIRED:
                    pending = event_data.get("pending_approvals", [])
                    yield create_sse_message_cn("approval_required", {
                        "pending_approvals": pending,
                        "message": "âš ï¸ éœ€è¦ç”¨æˆ·æ‰¹å‡†ä»¥ä¸‹æ“ä½œ",
                        "iteration": iteration_count + 1,
                        "timestamp": datetime.now().isoformat()
                    })
            
            # ç»Ÿè®¡æ±‡æ€»
            timing_stats["total"] = time.time() - total_start_time
            total_llm_time = sum(it["duration"] for it in timing_stats["llm_iterations"])
            total_tool_time = sum(tc["duration"] for tc in timing_stats["tool_calls"])
            slowest_tools = sorted(timing_stats["tool_calls"], key=lambda x: x["duration"], reverse=True)[:5]
            
            logger.info("-" * 60)
            logger.info("ğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
            logger.info(f"  â”œâ”€ æ€»è€—æ—¶: {format_duration(timing_stats['total'])}")
            logger.info(f"  â”œâ”€ åˆå§‹åŒ–: {format_duration(timing_stats['initialization'])} ({timing_stats['initialization']/timing_stats['total']*100:.1f}%)")
            logger.info(f"  â”œâ”€ æ¶ˆæ¯æ„å»º: {format_duration(timing_stats['message_building'])} ({timing_stats['message_building']/timing_stats['total']*100:.1f}%)")
            logger.info(f"  â”œâ”€ LLM è¿­ä»£: {format_duration(total_llm_time)} ({total_llm_time/timing_stats['total']*100:.1f}%) - {len(timing_stats['llm_iterations'])} æ¬¡")
            logger.info(f"  â””â”€ å·¥å…·è°ƒç”¨: {format_duration(total_tool_time)} ({total_tool_time/timing_stats['total']*100:.1f}%) - {len(timing_stats['tool_calls'])} æ¬¡")
            
            if slowest_tools:
                logger.info("  ğŸ¢ æœ€æ…¢çš„å·¥å…·è°ƒç”¨:")
                for i, tool in enumerate(slowest_tools, 1):
                    logger.info(f"     {i}. {tool['name']}: {format_duration(tool['duration'])}")
            
            logger.info("=" * 60)
            
            yield create_sse_message_cn("stream_end", {
                "success": True,
                "result": final_content
            })
            
            logger.info(f"âœ… æŸ¥è¯¢å®Œæˆï¼Œæ€»è€—æ—¶: {format_duration(timing_stats['total'])}")
            
        except Exception as e:
            total_time = time.time() - total_start_time
            logger.error(f"âŒ æ‰§è¡ŒæŸ¥è¯¢æ—¶å‡ºé”™ (è€—æ—¶ {format_duration(total_time)}): {e}", exc_info=True)
            yield create_sse_message_cn("error", {
                "success": False,
                "error": str(e)
            })
    
    def _execute_query_stream_text(
        self,
        question: str,
        system_prompt: Optional[str] = None,
        api_key: Optional[str] = None,
        model: Optional[str] = None,
        max_steps: int = 50
    ) -> Generator[str, None, None]:
        """
        æ‰§è¡ŒæŸ¥è¯¢å¹¶ä»¥æ˜“è¯»çš„çº¯æ–‡æœ¬æ ¼å¼æµå¼è¿”å›ç»“æœ
        ä¸“ä¸º curl ç­‰å‘½ä»¤è¡Œå·¥å…·ä¼˜åŒ–
        """
        total_start_time = time.time()
        tool_calls_collected = []
        timing_stats = {
            "initialization": 0,
            "message_building": 0,
            "llm_iterations": [],
            "tool_calls": [],
            "total": 0
        }
        iteration_count = 0
        current_tool_start_time = None
        current_tool_name = None
        llm_iteration_start_time = None
        
        def emit(text: str) -> str:
            return f"{text}\n"
        
        try:
            init_start = time.time()
            
            if api_key or model or max_steps != 50:
                self.config = None
                self.ai = None
            
            self.initialize(api_key=api_key, model=model, max_steps=max_steps)
            timing_stats["initialization"] = time.time() - init_start
            
            final_system_prompt = system_prompt or SYSTEM_PROMPT
            
            yield emit("=" * 70)
            yield emit(f"ğŸ” HolmesGPT æµå¼æŸ¥è¯¢")
            yield emit("=" * 70)
            yield emit(f"ğŸ“ é—®é¢˜: {question[:100]}")
            yield emit(f"â±ï¸  åˆå§‹åŒ–: {format_duration(timing_stats['initialization'])}")
            yield emit("-" * 70)
            yield emit("")
            
            msg_build_start = time.time()
            
            runbook_catalog = (
                self.merged_catalog if self.merged_catalog 
                else self.config.get_runbook_catalog()
            )
            
            messages = build_initial_ask_messages(
                console=self.console,
                initial_user_prompt=question,
                file_paths=None,
                tool_executor=self.ai.tool_executor,
                runbooks=runbook_catalog,
                system_prompt_additions=final_system_prompt if final_system_prompt else None
            )
            
            timing_stats["message_building"] = time.time() - msg_build_start
            
            sys_prompt = ""
            user_prompt = None
            msgs = []
            
            for msg in messages:
                if msg.get("role") == "system":
                    sys_prompt = msg.get("content", "")
                elif msg.get("role") == "user":
                    if user_prompt is None:
                        user_prompt = msg.get("content", "")
                    else:
                        msgs.append(msg)
                else:
                    msgs.append(msg)
            
            final_content = None
            llm_iteration_start_time = time.time()
            
            yield emit("ğŸ¤– å¼€å§‹ LLM è¿­ä»£...")
            yield emit("")
            
            for stream_event in self.ai.call_stream(
                system_prompt=sys_prompt,
                user_prompt=user_prompt,
                msgs=msgs if msgs else None
            ):
                event_type = stream_event.event
                event_data = stream_event.data
                
                if event_type == StreamEvents.START_TOOL:
                    tool_name = event_data.get("tool_name", "unknown")
                    current_tool_start_time = time.time()
                    current_tool_name = tool_name
                    yield emit(f"  ğŸ”§ [{iteration_count+1}] è°ƒç”¨å·¥å…·: {tool_name}")
                
                elif event_type == StreamEvents.TOOL_RESULT:
                    tool_name = event_data.get("name") or event_data.get("tool_name") or current_tool_name or "unknown"
                    result_dict = event_data.get("result", {})
                    description = event_data.get("description", "")
                    
                    tool_duration = 0
                    if current_tool_start_time:
                        tool_duration = time.time() - current_tool_start_time
                        timing_stats["tool_calls"].append({
                            "name": tool_name,
                            "duration": tool_duration,
                            "iteration": iteration_count + 1
                        })
                    
                    if isinstance(result_dict, dict):
                        status = result_dict.get("status", "unknown")
                        error_str = result_dict.get("error")
                    else:
                        status = "success"
                        error_str = None
                    
                    tool_info = {
                        "tool_name": tool_name,
                        "status": status,
                        "duration": tool_duration
                    }
                    tool_calls_collected.append(tool_info)
                    
                    status_icon = "âœ…" if status == "success" else "âŒ"
                    yield emit(f"  {status_icon} [{iteration_count+1}] å®Œæˆ: {tool_name} ({format_duration(tool_duration)})")
                    
                    if description:
                        yield emit(f"       ğŸ“‹ {description[:60]}")
                    
                    if error_str:
                        yield emit(f"       âš ï¸ é”™è¯¯: {error_str[:60]}")
                    
                    current_tool_start_time = None
                    current_tool_name = None
                
                elif event_type == StreamEvents.AI_MESSAGE:
                    content = event_data.get("content", "")
                    reasoning = event_data.get("reasoning", "")
                    
                    if reasoning:
                        yield emit(f"  ğŸ’­ [{iteration_count+1}] æ¨ç†:")
                        # å®Œæ•´è¾“å‡ºæ¨ç†å†…å®¹
                        for line in reasoning.split('\n'):
                            yield emit(f"     {line}")
                    
                    if content:
                        yield emit(f"  ğŸ’¬ [{iteration_count+1}] AI:")
                        # å®Œæ•´è¾“å‡º AI æ¶ˆæ¯å†…å®¹
                        for line in content.split('\n'):
                            yield emit(f"     {line}")
                
                elif event_type == StreamEvents.TOKEN_COUNT:
                    if llm_iteration_start_time:
                        iteration_duration = time.time() - llm_iteration_start_time
                        timing_stats["llm_iterations"].append({
                            "iteration": iteration_count + 1,
                            "duration": iteration_duration
                        })
                    
                    iteration_count += 1
                    llm_iteration_start_time = time.time()
                    
                    metadata = event_data.get("metadata", {})
                    usage = metadata.get("usage", {})
                    elapsed = format_duration(time.time() - total_start_time)
                    
                    if usage:
                        tokens = usage.get("total_tokens", 0)
                        yield emit(f"  ğŸ“Š [{iteration_count}] è¿­ä»£å®Œæˆ | Token: {tokens} | å·²ç”¨æ—¶: {elapsed}")
                    yield emit("")
                
                elif event_type == StreamEvents.CONVERSATION_HISTORY_COMPACTED:
                    yield emit(f"  ğŸ“¦ [{iteration_count+1}] å¯¹è¯å†å²å·²å‹ç¼©")
                
                elif event_type == StreamEvents.ANSWER_END:
                    final_content = event_data.get("content", "")
                    
                    if llm_iteration_start_time:
                        iteration_duration = time.time() - llm_iteration_start_time
                        timing_stats["llm_iterations"].append({
                            "iteration": iteration_count + 1,
                            "duration": iteration_duration
                        })
                    break
                
                elif event_type == StreamEvents.ERROR:
                    error_msg = event_data.get("msg", "æœªçŸ¥é”™è¯¯")
                    yield emit(f"  âŒ é”™è¯¯: {error_msg}")
            
            timing_stats["total"] = time.time() - total_start_time
            total_llm_time = sum(it["duration"] for it in timing_stats["llm_iterations"])
            total_tool_time = sum(tc["duration"] for tc in timing_stats["tool_calls"])
            slowest_tools = sorted(timing_stats["tool_calls"], key=lambda x: x["duration"], reverse=True)[:5]
            
            yield emit("-" * 70)
            yield emit("")
            yield emit("ğŸ¯ æœ€ç»ˆç­”æ¡ˆ:")
            yield emit("-" * 50)
            
            if final_content:
                for line in final_content.split('\n'):
                    yield emit(f"  {line}")
            
            yield emit("-" * 50)
            yield emit("")
            yield emit("ğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
            yield emit(f"  â”œâ”€ æ€»è€—æ—¶: {format_duration(timing_stats['total'])}")
            yield emit(f"  â”œâ”€ åˆå§‹åŒ–: {format_duration(timing_stats['initialization'])}")
            yield emit(f"  â”œâ”€ æ¶ˆæ¯æ„å»º: {format_duration(timing_stats['message_building'])}")
            
            if timing_stats['total'] > 0:
                llm_pct = total_llm_time / timing_stats['total'] * 100
                tool_pct = total_tool_time / timing_stats['total'] * 100
            else:
                llm_pct = tool_pct = 0
            
            yield emit(f"  â”œâ”€ LLM è¿­ä»£: {format_duration(total_llm_time)} ({llm_pct:.1f}%) - {len(timing_stats['llm_iterations'])} æ¬¡")
            yield emit(f"  â””â”€ å·¥å…·è°ƒç”¨: {format_duration(total_tool_time)} ({tool_pct:.1f}%) - {len(timing_stats['tool_calls'])} æ¬¡")
            
            if slowest_tools:
                yield emit("")
                yield emit("  ğŸ¢ æœ€æ…¢çš„å·¥å…·:")
                for i, tool in enumerate(slowest_tools, 1):
                    yield emit(f"     {i}. {tool['name']}: {format_duration(tool['duration'])}")
            
            yield emit("")
            yield emit("=" * 70)
            yield emit(f"âœ… å®Œæˆ! æ€»è€—æ—¶: {format_duration(timing_stats['total'])}")
            yield emit("=" * 70)
            
        except Exception as e:
            total_time = time.time() - total_start_time
            logger.error(f"âŒ æ‰§è¡ŒæŸ¥è¯¢æ—¶å‡ºé”™: {e}", exc_info=True)
            yield emit("")
            yield emit(f"âŒ é”™è¯¯: {str(e)}")
            yield emit(f"â±ï¸  è€—æ—¶: {format_duration(total_time)}")
    
    def get_tools_info(self) -> dict:
        """è·å–å¯ç”¨å·¥å…·ä¿¡æ¯"""
        self.initialize()
        
        tools = list(self.ai.tool_executor.tools_by_name.keys())
        toolsets = [{
            "name": toolset.name,
            "enabled": toolset.enabled,
            "status": toolset.status.value if hasattr(toolset.status, 'value') else str(toolset.status)
        } for toolset in self.ai.tool_executor.toolsets]
        
        return {
            "success": True,
            "total_tools": len(tools),
            "tools": sorted(tools),
            "toolsets": toolsets
        }
    
    def health_check(self) -> dict:
        """å¥åº·æ£€æŸ¥"""
        try:
            self.initialize()
            return {
                "status": "healthy",
                "config_loaded": self.config is not None,
                "ai_initialized": self.ai is not None
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e)
            }


# å…¨å±€æœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
_global_service: Optional[HolmesService] = None


def get_service() -> HolmesService:
    """è·å–å…¨å±€æœåŠ¡å®ä¾‹"""
    global _global_service
    if _global_service is None:
        _global_service = HolmesService()
    return _global_service

